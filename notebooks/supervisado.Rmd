---
title: "El problema de aprendizaje supervisado"
output: html_notebook
---

## Señal y ruido

Para abordar el problema de aprendizaje supervisado, es útil pensar nuestra variable de respuesta $y$ como una **función estocástica** de los datos $X$ (con variables de entrada $x_1, x_2, \ldots, x_p$). es decir: 

\begin{equation*}
  y = g(X)
\end{equation*}

donde $g$ es la función antes mencionada. Este supuesto general es aplicable a casi cualquier fenómeno que incluya aleatoridad, pero es difícil de manejar, por lo tanto, podemos proponer formas más específicas para la estocasticidad, por ejemplo:

\begin{align*}
  y =& f(X) + \epsilon \\
  y =& f(X) * \epsilon \\
  y =& f(X)^\epsilon \\
  \ldots
\end{align*}

donde $f$ es una **función deterministica** (llamada señal) y $\epsilon$ captura el componente aleatorio (ruido). Como podemos observar, el supuesto central es el de **separabilidad** de la señal y el ruido, y la forma funcional específica suele no ser tan importante. En general, en este curso supondremos $y = f(X) + \epsilon$.

**Ejemplo**

```{r}
library(ggplot2)
f <- function(x) {
  y <- x
  y <- y * 2 
  y[y > 0] <- y[y > 0] + 0.05*y[y > 0]^2
  y
}

x <- -20:20/2
eps <- rnorm(length(x), 0, 10)
y = f(x) + eps
ggplot() + 
  geom_line(aes(x=x, y=f(x))) +
  geom_point(aes(x=x, y=y)) +
  geom_segment(aes(x=x, y=f(x), xend=x, yend=y), color='red') +
  theme_light()
```

En esta gráfica, los puntos representan $y$, la señal está dada por la linea sólida, y en rojo se indica la magnitud de $\epsilon$ para cada punto. Podemos observar también la distribución de $\epsilon$

```{r}
ggplot() + geom_histogram(aes(x=eps), binwidth=5) + theme_light()
```

## Objetivo de un modelo

Dado que la señal $f$ y el ruido $\epsilon$ son independientes, el mejor resultado teórico posible para estimar $y$ es $\hat{y} = f(X)$

Bajo este esquema, el objetivo de un modelo de aprendizaje supervisado es **estimar la señal**. Si además, suponemos que nuestro ruido (o error) $\epsilon$ tiene valor esperado de 0 (Este supuesto es suficientemente general, pues siempre podemos añadir una consante a la señal). Entonces el problema de aprendizaje supervisado se vuelve estimar el **valor esperado de la función $g$, dados los datos X**, es decir:

\begin{equation}
  \hat{y} = \hat{E}[g | X] =  \hat{f}(X)
\end{equation}

En este contexto, **predecir** quiere decir generar valores $\hat{y}$ dados ciertos datos de entrada, es decir, evaluar el estimador en nuestros datos.

**Ejemplo**
```{r}
ggplot(mapping=aes(x=x, y=y)) +
  geom_point() +
  geom_smooth(method=lm, se=F, color='gray') + 
  geom_smooth(method=lm, se=F, formula=y~poly(x, 2), color="blue") +
  geom_smooth(method=lm, se=F, formula=y~poly(x, 15), color='red') +
  theme_light()
```

En esta gráfica tenemos tres estimadores distintos para los mismos datos. ¿Cuál resulta más apropiado?

## Complejidad de un modelo, sesgo vs varianza

En el ejemplo anterior, la línea en gris representa un estimador sencillo, mientras que la linea roja representa un estimador mucho más complejo. La línea en azul es un estimador ni tan sencillo, ni tan complejo. Lo que podemos observar es que hay un grado de complejidad "correcta" para descubrir la señal en este ejemplo.

Este fenómeno se da en general en el aprendizaje supervisado, y se conoce como el **_bias-variance tradeoff_**, ya que los modelos sencillos pueden tener alto **sesgo (bias)**, y los modelos complejos alta **varianza (variance)**. 

**Sesgo** se refiere a la capacidad del modelo de replicar puntos individuales, es decir, es una medida de que tan atinadas son nuestras estimaciones, un modelo con alto sesgo no toma en cuenta casos particulares o especiales, sino que aplica una medida similar para la mayoría de los puntos. Por otro lado, la **varianza** se refiere a la volatilidad del modelo, y nos dice que tan distintos serán nuestros resultados para un conjunto diferente de datos.

**Ejemplo**

Las siguientes gráficas muestra los estimadores presentados en la gráfica anterior, entrenados con diferentes muestras de los mismos datos

```{r}
sample_size <- 20
samples <-do.call(c, lapply(1:5, function(i) sample(1:length(x), sample_size, replace=FALSE)))
sample_idx <- rep(1:5, each=sample_size)
ggplot(mapping=aes(x=x[samples],y=y[samples],group=as.factor(sample_idx))) + 
  geom_smooth(method=lm, se=F, color="gray", fullrange=T) + 
  geom_point(aes(x=x, y=y, group=1)) +
  theme_light() +
  xlab("x") + 
  ylab("y")

```

```{r}
samples <-do.call(c, lapply(1:5, function(i) sample(1:length(x), sample_size, replace=FALSE)))
sample_idx <- rep(1:5, each=sample_size)
ggplot(mapping=aes(x=x[samples],y=y[samples],group=as.factor(sample_idx))) + 
  geom_smooth(method=lm, se=F, color="blue", formula=y~poly(x, 2)) + 
  geom_point(aes(x=x, y=y, group=1)) +
  theme_light() +
  xlab("x") + 
  ylab("y")

```

```{r}
samples <-do.call(c, lapply(1:5, function(i) sample(1:length(x), sample_size, replace=FALSE)))
sample_idx <- rep(1:5, each=sample_size)
ggplot(mapping=aes(x=x[samples],y=y[samples],group=as.factor(sample_idx))) + 
  geom_smooth(method=lm, se=F, formula=y~poly(x, 15), color="red") + 
  geom_point(aes(x=x, y=y, group=1)) + 
  theme_light() +
  xlab("x") + 
  ylab("y") +
  coord_cartesian(ylim = c(-40, 40))

```

En general, es importante buscar aquel modelo que balancee el sesgo con la varianza para capturar la mayoría de los patrones de nuestros datos, pero no introducir patrones innecesarios (alta varianza). Para esto necesitamos una manera de **evaluar un modelo**

## Métricas de error

Para poder evaluar nuestros modelos, necesitamos resumir de alguna manera sus **errores** a una o varias métricas que nos permitan compararlos. El error del modelo típícamente se refiere a la discrepancia entre las estimaciones o predicciones realizadas y los datos reales, y depende del conjunto de datos que usemos.

Una métrica popular es el error cuadrático medio, que se define de la siguiente manera:

\begin{equation}
  MSE(\hat{y}, y) = \displaystyle\frac{1}{n}\sum_{i=1}^n (y_i - \hat{y})^2
\end{equation}

¿Cuál de estos modelos tendría menor MSE? 

```{r}
ggplot(mapping=aes(x=x, y=y)) +
  geom_point() +
  geom_smooth(method=lm, se=F, color='gray') + 
  geom_smooth(method=lm, se=F, formula=y~poly(x, 2), color="blue") +
  geom_smooth(method=lm, se=F, formula=y~poly(x, 15), color='red') +
  theme_light()
```

Esto se da porque usamos **el mismo conjunto para entrenar que para evaluar**, esto tiende a sobreestimar el error estimado de nuestro modelo. Por lo mismo se sugiere separar nuestro conjunto de datos en tres:

* Conjunto de entrenamiento
* Conjunto de validación
* Conjunto de prueba

Esto nos permite estimar de una manera más certera el error.
