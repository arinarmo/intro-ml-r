---
title: "El problema de aprendizaje supervisado"
output: html_notebook
---

## Señal y ruido

Para abordar el problema de aprendizaje supervisado, es útil pensar nuestra variable de respuesta $y$ como una **función estocástica** de los datos $X$ (con variables de entrada $x_1, x_2, \ldots, x_p$). es decir: 

\begin{equation*}
  y = g(X)
\end{equation*}

donde $g$ es la función antes mencionada. Este supuesto general es aplicable a casi cualquier fenómeno que incluya aleatoridad, pero es difícil de manejar, por lo tanto, podemos proponer formas más específicas para la estocasticidad, por ejemplo:

\begin{align*}
  y =& f(X) + \epsilon \\
  y =& f(X) * \epsilon \\
  y =& f(X)^\epsilon \\
  \ldots
\end{align*}

donde $f$ es una **función deterministica** (llamada señal) y $\epsilon$ captura el componente aleatorio (ruido). Como podemos observar, el supuesto central es el de **separabilidad** de la señal y el ruido, y la forma funcional específica suele no ser tan importante. En general, en este curso supondremos $y = f(X) + \epsilon$.

**Ejemplo**

```{r}
library(ggplot2)
f <- function(x) {
  y <- x
  y <- y * 2 
  y[y > 0] <- y[y > 0] + 0.05*y[y > 0]^2
  y
}

x <- -20:20/2
eps <- rnorm(length(x), 0, 10)
y = f(x) + eps
ggplot() + 
  geom_line(aes(x=x, y=f(x))) +
  geom_point(aes(x=x, y=y)) +
  geom_segment(aes(x=x, y=f(x), xend=x, yend=y), color='red') +
  theme_light()
```

En esta gráfica, los puntos representan $y$, la señal está dada por la linea sólida, y en rojo se indica la magnitud de $\epsilon$ para cada punto. Podemos observar también la distribución de $\epsilon$

```{r}
ggplot() + geom_histogram(aes(x=eps), binwidth=5) + theme_light()
```

## Objetivo de un modelo

Dado que la señal $f$ y el ruido $\epsilon$ son independientes, el mejor resultado teórico posible para estimar $y$ es $\hat{y} = f(X)$

Bajo este esquema, el objetivo de un modelo de aprendizaje supervisado es **estimar la señal**. Si además, suponemos que nuestro ruido (o error) $\epsilon$ tiene valor esperado de 0 (Este supuesto es suficientemente general, pues siempre podemos añadir una consante a la señal). Entonces el problema de aprendizaje supervisado se vuelve estimar el **valor esperado de la función $g$, dados los datos X**, es decir:

\begin{equation}
  \hat{y} = \hat{E}[g | X] =  \hat{f}(X)
\end{equation}

## Complejidad de un modelo, sesgo vs varianza

## Métricas de error

